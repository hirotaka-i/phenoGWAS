{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post analysis script\n",
    "\n",
    "First created: 2018-08-06    \n",
    "By: Hirotaka Iwaki     \n",
    "\n",
    "\n",
    "This is to organize the meta-analysis results and provide some information for interpretations.    \n",
    "Trying to target two sets of variants.    \n",
    "1. The set of variants exceed the 5E-8 threshold in any of the meta-analysis.\n",
    "2. The 92 recently identified PD risk variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting interesting variants and get p-value for each outcomes\n",
    "1. Get the list of significant 5E-8 variants across outcomes\n",
    "2. Add newly identified 92 variants (Meta5)\n",
    "3. Add 31 variants in used in a recent PD-progression analysis.\n",
    "4. Take p-values of the variants from the analysis for each outcome\n",
    "5. Create cross-tab of $-log_{10}P$ for  [$variants \\times outcomes$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*The next cell should be uncommented  before starting the following*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# rm -rf post\n",
    "# mkdir -p post\n",
    "# rm -rf /data/LNG/Hirotaka/progGWAS/meta/rvtest/MMSE #MMSE_baseline is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# step1: Get the list of variants\n",
    "for ANALYSIS in \"surv\" \"rvtest\";do\n",
    "    mkdir post/$ANALYSIS\n",
    "    RES_FOLDER=\"/data/LNG/Hirotaka/progGWAS/meta/$ANALYSIS\"\n",
    "    for OUTCOME in $(ls $RES_FOLDER);do\n",
    "        awk '$10 < 5e-8 {print $1}' $RES_FOLDER/$OUTCOME/meta2.tbl > post/$ANALYSIS/sigV_\"$OUTCOME\".list\n",
    "    done\n",
    "done\n",
    "rm -f post/sigV.list\n",
    "cat post/*/sigV*.list | sort -u  >> post/sigV.list\n",
    "#  rm post/sigV_*.list\n",
    "echo 'complete step1'\n",
    "\n",
    "# step2,3: Get the list of variants from Meta5, PD-progressio analysis \n",
    "tail -n +2 data/Meta5.tab | cut -f1 > post/Meta5.list\n",
    "awk 'BEGIN{FS=\",\"}/NC_/{print \"chr\"$5\":\"$6}' data/PriorSNPsGRCh37.csv > post/snp31.list\n",
    "cat post/Meta5.list post/sigV.list post/snp31.list | uniq > post/allV.list\n",
    "echo \"complete step2,3\"\n",
    "\n",
    "# step4: Get P of the variants from each result (regardless of significant or not)\n",
    "for ANALYSIS in surv rvtest;do\n",
    "    RES_FOLDER=/data/LNG/Hirotaka/progGWAS/meta/$ANALYSIS\n",
    "    for OUTCOME in $(ls $RES_FOLDER);do\n",
    "       grep -f post/allV.list $RES_FOLDER/$OUTCOME/meta2.tbl |\\\n",
    "            cut -f 1,10 > post/$ANALYSIS/allV_\"$OUTCOME\".tbl\n",
    "    done\n",
    "done\n",
    "echo 'complete step4'\n",
    "\n",
    "# step5: P Cross-tabulated over outcomes\n",
    "## R program detect folders in \"post\" as a analysis\n",
    "echo '\n",
    "library(dplyr);library(data.table)\n",
    "ANALYSES = list.dirs(\"./post/\", full.names=F)[-1] # delete self\n",
    "for (ANALYSIS in ANALYSES){\n",
    "    ALLFILES = list.files(paste(\"post/\", ANALYSIS, \"/\", sep=\"\"))\n",
    "    FILES=ALLFILES[grepl(\"allV_\",ALLFILES)]\n",
    "    OUTCOMES = substring(FILES, 6, nchar(FILES)-4)\n",
    "    for(i in 1:length(OUTCOMES)){\n",
    "        dt_i = try(fread(paste(\"post\",ANALYSIS,FILES[i],sep=\"/\"), header=F), silent=T)\n",
    "        if(class(dt_i)[1]==\"try-error\"){next}\n",
    "        names(dt_i) = c(\"V1\",paste(OUTCOMES[i], ANALYSIS, sep=\"_\"))\n",
    "        if(!exists(\"dt1\")){dt1 = dt_i}else{    dt1 = full_join(dt1, dt_i, by = \"V1\")}\n",
    "    }\n",
    "    cat(paste(\"complete step5 -\", ANALYSIS, \"\\n\"))\n",
    "}\n",
    "dt2 = dt1 %>% mutate_at(vars(names(dt1)[2:ncol(dt1)]), funs(-log10(.)))\n",
    "write.table(dt2, paste(\"post/allV.tab\", sep=\"\"), row.names = F, quote = F, sep = \"\\t\")\n",
    "' > post/_CrossTab.R\n",
    "module load R\n",
    "Rscript --vanilla post/_CrossTab.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the annotation information\n",
    "\n",
    "1. get rsID from variants' location and reference file.\n",
    "2. get informaion from dbSNP using rsID.\n",
    "3. join the information to the original cross-tab and sort by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Step1:  get rsIDs for the variants\n",
    "sed 's/^chr//g' post/allV.tab | (head -n 1 - && tail -n +2 - | LANG=C sort) |\\\n",
    "    LANG=C join -t$'\\t' --header - ../tools/rs_37_sort.txt > post/allV_RS.tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: pull information from dbSNP\n",
    "data = pd.read_table(\"post/allV_RS.tab\")\n",
    "IDs = data[\"ID\"] # dbSNP ID\n",
    "\n",
    "SYMBOLs = []\n",
    "SOTERMs = []\n",
    "FXNCLASSs=[]\n",
    "for ID in IDs:\n",
    "    print(ID)\n",
    "    response = requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=snp&id=' + ID[2:] + '&report=XML') \n",
    "    html_str = response.text \n",
    "    bs = BeautifulSoup(html_str, \"html5lib\") \n",
    "    try:\n",
    "        SYMBOL = bs.fxnset['symbol'] \n",
    "        SOTERM = bs.fxnset['soterm']\n",
    "        FXNCLASS = bs.fxnset['fxnclass']\n",
    "    except (TypeError, KeyError):\n",
    "        if len(bs.find_all('fxnset'))>1:\n",
    "            print(\"search from the second tag\")\n",
    "            SYMBOL = bs.find_all('fxnset')[1]['symbol'] \n",
    "            SOTERM = bs.find_all('fxnset')[1]['soterm']\n",
    "            FXNCLASS = bs.find_all('fxnset')[1]['fxnclass']\n",
    "        else:\n",
    "            SYMBOL = \"NA\" \n",
    "            SOTERM = \"NA\"\n",
    "            FXNCLASS = \"NA\"\n",
    "    SYMBOLs.append(SYMBOL)\n",
    "    SOTERMs.append(SOTERM)\n",
    "    FXNCLASSs.append(FXNCLASS)\n",
    "    print (SYMBOL, SOTERM, FXNCLASS)\n",
    "    time.sleep(1/3) # three requests per second (Guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step3 join the cross-tab with the new information\n",
    "data2= data.assign(SYMBOL=SYMBOLs, SOTERM=SOTERMs, FXNCLASS=FXNCLASSs)\n",
    "## ordering the \n",
    "forIdx = data2[\"V1\"].str.split(\":\", expand=True).applymap(int).sort_values(by=[0, 1])\n",
    "df = data2.reindex(forIdx.index)\n",
    "df.head()\n",
    "df.to_csv(\"outputs/allV_info.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823, 38)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"outputs/allV_info.csv\", sep='\\t', index_col=0, keep_default_na=False, na_values=\"\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.iloc[:, 1:(df.shape[1]-6)]\n",
    "df_out.index = df['V1'] + \"[\" + df['FXNCLASS'] + \"] \" + df['SYMBOL']\n",
    "fig=plt.figure(figsize=(15, 80), dpi= 80, facecolor='w', edgecolor='k')\n",
    "sns.heatmap(df_out,cmap='RdBu_r', vmin=0, center=1.301, vmax=7.301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I look at the MMESE_slope, it has many hits but they don't overlap with other relevant outcomes (e.g. DEMENTIA_surv). I used a new algorithm for a slope analysis (data projection to a complementary orthogonal matirx) so I need to double check it with glmm analysis. (It would take a few days).    \n",
    "To get an idea from more confirmative results, ** I will remove variants only significant in the slope outcomes. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_slope = df.loc[:,[i for i in list(df) if not \"slope\" in i]]\n",
    "df_wo_slope_sig = (df_wo_slope.iloc[:, 1:(df_wo_slope.shape[1]-6)] > 7.301).sum(axis=1)\n",
    "df_wo_slop_less = df_wo_slope.loc[df_wo_slope_sig>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df_wo_slop_less\n",
    "dft_out = dft.iloc[:, 1:(dft.shape[1]-6)]\n",
    "dft_out.index = dft['V1'] + \" [\" + dft['FXNCLASS'] + \"] \" + dft['SYMBOL']\n",
    "dft_out.head()\n",
    "fig=plt.figure(figsize=(10,60), dpi= 80, facecolor='w', edgecolor='k')\n",
    "sns.heatmap(dft_out,cmap='RdBu_r', vmin=0, center=1.301, vmax=7.301)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
